\section{Experiments}
\subsection{Datasets}
\subsubsection{Stanford Sentiment Treebank} \label{sec:sst}
Standford Sentiment Treebank dataset~\cite{socher2013recursive} was used to evaluate our model.
In total, Standford Sentiment Treebank contains 11,855 sentences.
The dataset was split into training, validation and test set which contains 8544, 1101 and 2210 sentences respectively.
In this dataset, every sentence was parsed using Stanford (constituency) parser~\cite{socher2013recursive}, each phrase which is spanned by any sub-tree of the parse tree is then labeled with  a sentiment label.
There are total 215,154 labeled phrases in the whole dataset.
For training a Recurrent Neural Network on this dataset, any phrase which is spanned by a labeled node is treated as a training example. 
\paragraph{Fine-grained setting} Any sentiment label belongs to one in 5 classes: "Positive", "Somewhat Positive", "Neutral", "Somewhat Negative" and "Negative".
\paragraph{Binary setting} All "Neutral" sentences are removed.
For the remaining sentences, "Somewhat Positive" is merged into "Positive", similarly, "Somewhat Negative" applied on "Somewhat Negative" is merged into "Negative".
Due to all "Neutral" sentences have been removed, there are 6920/872/1821 sentences remained in training/validation/test set.
\subsubsection{Amazon Reviews}\label{sec:amazon}
Amazon Reviews~\cite{amazon-reviews} is a gigantic review dataset
which contains 142.8 million reviews from Amazon spanning May 1996 - July 2014\footnote{\url{http://jmcauley.ucsd.edu/data/amazon/}}.
Each review contains product review (rating, text, helpfulness vote) and metadata (descriptions, category information, price, brand, and image features).
The dataset is partitioned into 24 categories (e.g. ``Books'', ``Electronics'', ``Office Products'', ``Movies and TV'').
\subsection{Setups}
\subsubsection{Glove Amazon}
Steps for preprocessing Amazon dataset for training word vectors using Glove\footnote{Publicly available on Github \url{https://github.com/stanfordnlp/GloVe}}:
\begin{enumerate}
	\item For retraining Glove vectors, we only used some partitions of Amazon Reviews dataset which includes:  ``Amazon Movies and TV'' (7,850,072 reviews)~\cite{mcauley2013hidden}, ``Books'' (22,507,155 reviews) and the new ``Movies and TV'' (4,607,047 reviews)~\cite{McAuleyTSH15}~\cite{HeM16}.
	\item All the reviews were grouped by product-ID ( "asin" keyword in the JSON schema of the dataset).
	\item In each product-ID group, the reviews were sorted increasingly by their ratings ("overall" keyword in the JSON schema of the dataset).
	\item All the reviews were dumped into a plain text file.
	\item The text file produced from the previous step was tokenized using Stanford Tokenizer~\cite{tokenizerpart}.
\end{enumerate}

The reason for doing step 2 and step 3 is because there is no definition of end-of-document in Glove model, which means words which appear in the beginning part of a document will be included in the context of words in the last part of the previous document which leads to noise in training data. Step 2 and 3 help us to mitigate this problem.

We set $x_{max} = 100$, vector size to 300, windows size to 20 and the minimum number of word occurrences to be included in the vocabulary to 5.
The training process took the plain text file produced by the preprocessing steps as its input.
In total, the size of the corpus is 4.7 billions tokens.
After the training process, the resulting word embeddings has vocabulary size of 1,734,244.
We named this word embeddings Glove Amazon.
\subsubsection{Model Variations}
We did experiment with different variations of our model.
\begin{description}
	\item[CNN-Tree-LSTM] Our basic model which has only one input channel. 
	This input channel is initialized using the standard Glove vectors\footnote{Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download) publicly available at \url{https://nlp.stanford.edu/projects/glove/}}.
	\item[CNN-Tree-LSTM (Glove Amazon)] CNN-Tree-LSTM but its input channel is initialized using Glove Amazon.
	\item [2-channel CNN-Tree-LSTM] A model with two input channels, one is initialized using the standard Glove vectors while Glove Amazon is used for the other.
\end{description}
We do several more experiment for the sake of comparison.
\begin{description}
	\item[Constituency Tree-LSTM (Glove Amazon)] In this model, Glove Amazon is used to replace the standard Glove vector for initializing word embedding layer of Constituency Tree-LSTM.
	Apart from that, the whole training process and hyper-parameters of Constituency Tree-LSTM are kept unchanged.
	\item[CNN-LSTM] Similar to CNN-Tree-LSTM but the Constituency Tree-LSTM module is replaced by a LSTM unit.
	This model have only one input channel which is initialized using the standard Glove vectors.
	\item [CNN-LSTM (Glove Amazon)] CNN-LSTM but its input channel is initialized using Glove Amazon.
	\item [2-channel CNN-LSTM] CNN-LSTM but having two input channels, one is initialized with the standard Glove vectors and the other is initialized with Glove Amazon.
\end{description}
We index all our experimented models along with their number of parameters in Table.\ref{table:paramtable}.
\begin{table}[H]
	\centering
	\caption{Number of trainable parameters of experimented models}
	\label{table:paramtable}
	\begin{tabular}{lll}
		Model & r & \(\left\vert{\theta}\right\vert\) \\ \hline
		Constituency Tree-LSTM   & 150         & 316,800          \\
		CNN-LSTM                 & 168         & 489,347          \\
		CNN-Tree-LSTM            & 150         & 482,153          \\
		2-channel CNN-LSTM       & 168         & 729,347          \\
		2-channel CNN-Tree-LSTM  & 150         & 722,153
	\end{tabular}
\end{table}
\subsubsection{Hyper-parameters and Training}
Our models was trained using AdaGrad~\cite{duchi2011adaptive} with learning rate of $\{0.1,~ 0.05,~ 0.01\}$, L2 regularization strength of $\{1e^{-3},~ 1e^{-4}, ~ 1e^{-5} \}$ and batch size of 25.
Word vectors are updated with learning rate $\alpha$ of $\{0.1,~0.05, ~0.01\}$.

We tried a variety of convolution filters combination.
For the case all the filters having only a single kernel size, we performed grid search on $\{100, 200, 300\}$ number of kernels of size $\{3, 5\}$.
If a convolution layer has two different kernel sizes, we tried with the number of kernels of $\{100, 200\}$ for each kernel size in $\{3, 5\}$.
We regularized the convolution layers with input dropout rate of 0.5 and output dropout rate of 0.2 in addition to dropout of rate 0.5 at output layer.

We found that 100 filters of size 3 and 100 filters of size 5 yield better results compared to single filters size or the number of filters larger than 200. 
Additionally, training with Adagrad's learning rate of 0.01 and word vectors' learning rate 0.1 give the best result. 
Our models were trained for 60 epochs.
\subsection{Results}
\begin{table*}[]
	\centering
	\caption[Experiment result on SST]{Experiment results of models evaluated on Stanford Sentiment Treebank with binary setting.
		The models which have both data of mean(std) and max are models which have been evaluated by us.
		For these models, we report mean, standard deviation and max of 5 runs.
		If a model has data of only mean(std) or only max, the data was taken from its originated research paper.
		If the data of both mean(std) and max is missing, the model was not evaluated yet.
		\textit{(*): Result reported by the original paper.}}
	\label{table:experimentresult}
	\begin{tabular}{c|l|ll|ll}
		   ~ & ~  & \multicolumn{2}{c|}{\textbf{Binary}} & \multicolumn{2}{c}{\textbf{Fine-grained}}  \\
		\cline{3-6}
		\textbf{Block}    & \textbf{Model}  & \textbf{Mean(std)} & \textbf{Max} & \textbf{Mean(std)} & \textbf{Max}  \\
		\Xhline{3\arrayrulewidth}
		\Xhline{3\arrayrulewidth}
		
		\multirow{4}{*}{A} & CNN-non-static~\cite{KimCNN} & - & 87.20 & - & 48.00 \Tstrut \\
		& CNN-multichannel~\cite{KimCNN} & - & 88.10 & - & 47.40\\
		& DCNN~\cite{DCNN} & - & 86.80 & - & 48.50 \\
		& MVCNN~\cite{2-layer-cnn} & - & 89.40 & - & 49.60 \\
		\hline
		\multirow{5}{*}{B} & LSTM~\cite{treeLSTM}   & 84.9 (0.6) & - & 46.40 (1.1)& - \\
		& BiLSTM~\cite{treeLSTM}  & 87.50 (0.5) & - & 49.1 (1.0) & -  \\
		& 2-layer LSTM~\cite{treeLSTM} & 86.30 (0.60) & - & 46.00 (1.30) & -\\
		& 2-layer Bidirectional LSTM~\cite{treeLSTM} & 87.20 (1.00) & - & 48.50 (1.00) & -\\
		& DMN~\cite{attention-gru} & - & 88.60 & - & 52.10 \\
		\hline
		\multirow{5}{*}{C} & RNTN~\cite{socher2013recursive}  & - & 85.40 & - & 45.70 \\
		& DRNN~\cite{IrsoyDRNN} & - & 86.60 & - & 49.80  \\
		& TE-RNTN~\cite{tag-embedding-rnn} & - & 87.70 & - & 48.90\\
		& Dependency Tree-LSTM~\cite{treeLSTM}  & 85.70 (0.40)  & - & 48.40 (0.4) & - \\
		& Constituency Tree-LSTM~\cite{treeLSTM} & 88.00 (0.30)    &  - & 51.0 (0.5) & - \\
		\hline
		\multirow{3}{*}{D} & GICF~\cite{group-instance} & - & 85.70 & - & - \\
		& Paragraph-Vec~\cite{ParagraphVec} & - & 87.80 & - & 48.70 \\
		& LSTM (PARAGRAM-SL999)~\cite{wieting2015towards} & 87.98 (0.46) & 88.50 (89.20)* &   &  
		\\
		\hline
		\multirow{2}{*}{E}  & CNN GRU ~\cite{cnn-rnn}                    & 89.13 (0.29)  &  89.61 (89.95)*    \\
		& CNN LSTM ~\cite{cnn-rnn}                    & 89.43 (0.28)  & 89.72 (89.56)* & 52.03 (0.77) & 52.56 (51.50)* \Bstrut    \\
		\Xhline{3\arrayrulewidth}
		\Xhline{3\arrayrulewidth}
		\multirow{6}{*}{F} & Constituency Tree-LSTM ~\cite{treeLSTM} (Glove Amazon) & 88.85 (0.44) & 89.35 \Tstrut \\
		 & CNN LSTM                                 & 89.10 (0.39)  & 89.40  \\
		  & CNN LSTM (Glove Amazon)                  &   &   \\
		& 2 Channel CNN LSTM                        & 89.54    (0.22) & 89.79    \\
		 & CNN Tree-LSTM                            & 88.82 (0.13) & 88.92 & 51.35 (1.45) & 52.94 \\
		& CNN Tree-LSTM (Glove Amazon)             & 88.96 (0.24) & 89.18 & 51.51 (0.99) & 52.8 \\
		& 2 Channel CNN Tree-LSTM  &\textbf{89.69 (0.36)} & \textbf{90.12}  & \textbf{52.46 (0.55)} & \textbf{53.03}  \\
	\end{tabular}
\end{table*}
Experiment results are summaries in Table \ref{table:experimentresult}.
Table \ref{table:experimentresult} is divided into two parts.
The first part is from Block A to E, which contains all baselines model.
The second part is Block F, which contains all models which were proposed and evaluated by us.
\begin{description}
	\item[Block A] contains Multilayer Convolution Neural Networks models.
	CNN-non-static and CNN-multichannel~\cite{KimCNN} are single layer CNN.
	DCNN~\cite{DCNN} and MVCNN~\cite{2-layer-cnn} are multilayer CNN, with MVCNN is a very large model  which has 2 layers, 5 word embeddings channels and unsupervised pre-train using the method of Sentence Encoding.
	\item[Block B] contains sequential/recurrent models.
	LSTM~\cite{originLSTM}, BiLSTM~\cite{GravesLSTM}, 2-layer LSTM~\cite{GravesLSTM} and 2-layer Bidirectional LSTM~\cite{GravesLSTM} have been described in Sec.\ref{sec:RNN}.
	DMN~\cite{attention-gru} is a sophisticated model used GRU with attention mechanism and episodic memory.
	\item[Block C] contains models which belong to the family of Recursive Neural Networks (tree-structured model).
	RNTN~\cite{socher2013recursive} is the first recursive neural network to successfully apply on sentence-level sentiment analysis (Stanford Sentiment Treebank).
	It was inspired by the idea that natural languages have recursive structure, to understand a sentence we must understand its phrases and, to understand a phrase, we must understand its words.
	DRNN~\cite{IrsoyDRNN} is a multilayered extension of RNTN.
	TE-RNTN is also an extension of RNTN which utilize the local syntactic information at each node of a sentence's parse tree.
	Dependency and Constituency Tree-LSTM~\cite{treeLSTM} are tree-structured versions of LSTM which have been described in Sec.\ref{sec:treelstm}.
	\item[Block D] contains transfer learning methods, which utilized a large amount of data other than Stanford Sentiment Treebank.
	GICF~\cite{group-instance} is an attempt to learn to classify sentiments of sentences (in Stanford Sentiment Treebank) from training dataset which contains only document-level sentiment labels.
	Paragraph-Vec~\cite{ParagraphVec} is a method that learns to encode any sequence of words into a vector with the purpose of maximizing the likelihood of words which appear in that sequence given the encoding vector.
	LSTM (PARAGRAM-SL999)~\cite{wieting2015towards} is a LSTM models with word embeddings layer initialized by PARAGRAM-SL999, word vectors trained on a large paraphrase dataset (PPDB~\cite{ganitkevitch2013ppdb}) % which was trained using a large paraphrase dataset (PPDB~\cite{ganitkevitch2013ppdb}).
	In our experiments with LSTM (PARAGRAM-SL999), we used the implementation and pre-trained word vectors which are publicly available on the author website\footnote{\url{http://ttic.uchicago.edu/~wieting/}}.
	\item[Block E] contains models which combine Convolution Neural Networks and Recurrent Neural Networks.
	CNN GRU and CNN LSTM~\cite{cnn-rnn} have been described in detail in Sec.\ref{cnn-rnn}.
	In our experiments with CNN GRU and CNN LSTM, we used the implementation publicly published by the authors\footnote{\url{https://github.com/ultimate010/crnn}}.
\end{description}
\subsection{Discussion}
In Block G, Constituency Tree-LSTM using Glove Amazon largely outperformed Constituency Tree-LSTM using Glove Common Crawl even though it was trained on significantly larger dataset (840B tokens) compared to our preprocessed Amazon Reviews (4.7B tokens).

Compared to Glove Amazon method, originally,  Paragraph-Vec~\cite{ParagraphVec} cannot be fine-tuning in a supervised manner.
We also used PARAGRAM-SL999 for initializing the word embeddings layer of Constituency Tree-LSTM,
the results (of 8 runs with mean 87.175\% and standard deviation 0.69) were not as good as those of Glove Common Crawl.

These results support our hypothesis that by training word embeddings on review documents, especially movie or book reviews, we can capture more rare words and also the different way people use words (or different word relationships) when they express their opinions on movies or books.

The fact that CNN Tree-LSTM outperforms Constituency Tree-LSTM~\cite{treeLSTM} supports our hypothesis on the benefits of combining convolution layers with Tree-LSTM.
Due to using similar architects, CNN LSTM was comparable with CNN GRU and CNN LSTM~\cite{cnn-rnn}. \label{unproved:cnn-treelstm-overfit}
CNN Tree-LSTM performed worst than CNN LSTM, the reason might because of over-fitting.
We will prove this hypothesis by looking at the plots of error rate on training and validation set of these two models.

\label{proved:Amazon-adv-Common}
We have seen that Glove Amazon improved Tree-LSTM in Sec.\ref{fact:glove-amazon-improve-tree}.
In these experiments (Block I), Glove Amazon also improved CNN Tree-LSTM.
We can conclude that Amazon Glove captured some good\footnote{good for the task of sentiment analysis of movie reviews} features that does not exist or hardly be extracted in Glove Common Crawl.

According to Table.\ref{table:paramtable}, the number of parameters of 2 Channel CNN Tree-LSTM (722,153) is much larger than that of CNN Tree-LSTM (482,153), which makes it more likely for 2 Channel CNN Tree-LSTM to over-fit the training data.
However, in fact, 2 Channel CNN Tree-LSTM was able to archive much higher accuracy than both CNN Tree-LSTM (Glove Amazon) and CNN Tree-LSTM (Glove Common Crawl).\label{proved:Common-syn-Amazon}
The result proves that there are some features only appearing in Glove Common Crawl or when combining both Glove Amazon and Glove Common Crawl.
Otherwise, there would not be any improvement.
