\section{Experiments}
\subsection{Datasets}
\subsubsection{Stanford Sentiment Treebank} \label{sec:sst}
Standford Sentiment Treebank dataset~\cite{socher2013recursive} was used to evaluate our model.
In total, Standford Sentiment Treebank contains 11,855 sentences.
The dataset was split into training, validation and test set which contains 8544; 1101 and 2210 sentences respectively.
In this dataset, every sentence was parsed using Stanford (constituency) parser~\cite{socher2013recursive}, each phrase which is spanned by any sub-tree of the parse tree is then labeled with  a sentiment label.
There are total of 215,154 labeled phrases in the whole dataset.
For training a Recurrent Neural Network on this dataset, any phrase which is spanned by a labeled node is treated as a training sample.
\paragraph{Fine-grained setting} Any sentiment label belongs to one of 5 classes: ``Positive'', ``Somewhat Positive'', ``Neutral'', ``Somewhat Negative'' and ``Negative''.
\paragraph{Binary setting} All ``Neutral'' sentences are removed.
For the remaining sentences, ``Somewhat Positive'' is merged into ``Positive'', similarly, ``Somewhat Negative'' is merged into ``Negative''.
As all ``Neutral'' sentences have been removed, there are 6920/872/1821 sentences remained in training/validation/test set.
\subsubsection{Amazon Reviews}\label{sec:amazon}
Amazon Reviews~\cite{amazon-reviews} is a gigantic review dataset
which contains 142.8 million reviews from Amazon spanning May 1996 - July 2014\footnote{\url{http://jmcauley.ucsd.edu/data/amazon/}}.
Each review contains product review (rating, text, helpfulness vote) and metadata (descriptions, category information, price, brand, and image features).
The dataset is partitioned into 24 categories (e.g. ``Books'', ``Electronics'', ``Office Products'', ``Movies and TV'').
\subsection{Setups}
\subsubsection{Glove Amazon}\label{sec:glove-amazon}
We have five steps to preprocess Amazon dataset for training word vectors using Glove\footnote{Publicly available on Github \url{https://github.com/stanfordnlp/GloVe}}:
\begin{enumerate}
	\item For retraining Glove vectors, we only used some partitions of Amazon Reviews dataset which includes:  ``Amazon Movies and TV'' (7,850,072 reviews)~\cite{mcauley2013hidden} and ``Books'' (22,507,155 reviews)~\cite{McAuleyTSH15}~\cite{HeM16}.
	\item All the reviews were grouped by product-ID ( "asin" keyword in the JSON schema of the dataset).
	\item In each product-ID group, the reviews were sorted increasingly by their ratings ("overall" keyword in the JSON schema of the dataset).
	\item All the reviews were dumped into a plain text file.
	\item The text file produced from the previous step was tokenized using Stanford Tokenizer~\cite{tokenizerpart}.
\end{enumerate}

The reason for doing step 2 and step 3 is because there is no definition of end-of-document in Glove model, which means words which appear in the beginning part of a document will be included in the context of words in the last part of the previous document which leads to noise in training data. Step 2 and 3 help us to mitigate this problem.

We set $x_{max} = 100$, vector size to 300, windows size to 20 and the minimum number of word occurrences to be included in the vocabulary to 5.
The training process took the plain text file produced by the preprocessing steps as its input.
In total, the size of the corpus is 4.7 billions tokens.
After the training process, the resulting word embedding has vocabulary size of 1,734,244.
We named this new word embedding Glove Amazon.
\subsubsection{Experiment Descriptions}
We did experiment with Glove Amazon and different variations of our model.
\begin{description}
	\item[Constituency Tree-LSTM (Glove Amazon)] In this model, Glove~Amazon is used to replace the standard Glove vector for initializing word embedding layer of Constituency Tree-LSTM.
	Apart from that, the whole training process and hyper-parameters of Constituency Tree-LSTM~\cite{treeLSTM} are kept unchanged.
	\item[CNN-Tree-LSTM] Our basic model which has only one input channel.
	This input channel is initialized using the standard Glove vectors.
	\item[CNN-Tree-LSTM (Glove Amazon)] CNN-Tree-LSTM with word vectors initialized from Glove Amazon.
	\item [2-channel CNN-Tree-LSTM] Model's CNN layerer have two input channels from two different word embedding matrices, which are initialized from Glove Common Craw and Glove Amazon, respectively.
	\item[CNN-LSTM] Similar to CNN-Tree-LSTM. However, we replaced Constituency Tree-LSTM module by a LSTM unit.
	This model has only one input channel initialized from the standard Glove vectors.
	\item [CNN-LSTM (Glove Amazon)] CNN-LSTM with word vectors initialized from Glove Amazon.
	\item [2-channel CNN-LSTM] A CNN-LSTM model with two input channels at CNN layers. The CNN layers is similar to CNN layer in 2-channel CNN-Tree-LSTM model.
\end{description}

Since the cost functions of neural networks are non-convex and algorithms used to train neural networks are only able to find local optimum, different runs of one models can converge in different local optimums depend on the initialized parameters of the models.  
Base on the evaluating method used by Tai et al.~\cite{treeLSTM}, we evaluated the aboves models based on mean, standard deviation of 5 runs.
In addition, the maximum accuracy among 5 runs is also reported.

We index all our experimented models along with their number of parameters in Table.\ref{table:paramtable}.
\begin{table}[H]
	\centering
	\caption{Size of memory cell \(r\) and number of trainable parameters \(\left\vert{\theta}\right\vert\) of our models.
	}
	\label{table:paramtable}
	\begin{tabular}{|l|l|l|}
		\hline
		Model & \(r\) & \multicolumn{1}{|c|}{\(\left\vert{\theta}\right\vert\)}\\ \hline
		CNN-LSTM                 & 168         & 489,347          \\
		CNN-Tree-LSTM            & 150         & 482,153          \\
		2-channel CNN-LSTM       & 168         & 729,347          \\
		2-channel CNN-Tree-LSTM  & 150         &
		722,153 \\
		\hline
	\end{tabular}
\end{table}
\subsubsection{Hyper-parameters and Training}
Our models was trained using AdaGrad~\cite{duchi2011adaptive} with learning rate of $\{0.1,~ 0.05,~ 0.01\}$, L2 regularization strength of $\{1e^{-3},~ 1e^{-4}, ~ 1e^{-5} \}$ and batch size of 25.
Word vectors are updated with learning rate $\alpha$ of $\{0.1,~0.05, ~0.01\}$.
We regularized the convolution layers with input dropout rate of 0.5 and output dropout rate of 0.2 in addition to dropout of rate 0.5 at output layer.

We found that 100 filters of size 3 and 100 filters of size 5 yield better results compared to single filters size or the number of filters larger than 200.
Additionally, training with Adagrad's learning rate of 0.01 and word vectors' learning rate 0.1 give the best result.
Our models were trained for 60 epochs.
\subsection{Results}\label{sec:result}
\begin{table*}[]
	\centering
	\caption[Experiment result on SST]{
		Experiment results of models evaluated on Stanford Sentiment Treebank.
		The accuracies of models in blocks A to E are taken from their original papers.
		We highlight the best results among our models and underline the state-of-the-art model fine-grained setting.
		\textit{(*): Mean and standard deviation of 5 runs.}
	}
	\label{table:experimentresult}
	\begin{tabular}{|c|l|ll|ll|}
		\hline
		   \textbf{Block} & \textbf{Model}  & \multicolumn{2}{c|}{\textbf{Binary}} & \multicolumn{2}{c|}{\textbf{Fine-grained}}  \\
		\Xhline{3\arrayrulewidth}
		\Xhline{3\arrayrulewidth}

		\multirow{4}{*}{A} & CNN-non-static~\cite{KimCNN} & \multicolumn{2}{c|}{87.2} & \multicolumn{2}{c|}{48.0} \Tstrut \\
		& CNN-multichannel~\cite{KimCNN} & \multicolumn{2}{c|}{88.1} & \multicolumn{2}{c|}{47.4} \\
		& DCNN~\cite{DCNN} & \multicolumn{2}{c|}{86.8} & \multicolumn{2}{c|}{48.5} \\
		& MVCNN~\cite{2-layer-cnn} & \multicolumn{2}{c|}{89.4} & \multicolumn{2}{c|}{49.6} \\
		\hline
		\multirow{6}{*}{B} & LSTM~\cite{treeLSTM}   & \multicolumn{2}{c|}{84.9 (0.6)*} & \multicolumn{2}{c|}{46.4 (1.1)*} \\
		& BiLSTM~\cite{treeLSTM}  & \multicolumn{2}{c|}{87.5 (0.5)*} & \multicolumn{2}{c|}{49.1 (1.0)*}   \\
		& 2-layer LSTM~\cite{treeLSTM} & \multicolumn{2}{c|}{86.3 (0.6)*} & \multicolumn{2}{c|}{46.0 (1.3)*} \\
		& 2-layer Bidirectional LSTM~\cite{treeLSTM} & \multicolumn{2}{c|}{87.2 (1.0)*} & \multicolumn{2}{c|}{48.5 (1.0)*} \\
		& DMN~\cite{attention-gru} & \multicolumn{2}{c|}{88.6 } & \multicolumn{2}{c|}{52.1} \\
		\hline
		\multirow{6}{*}{C} & RNTN~\cite{socher2013recursive}  & \multicolumn{2}{c|}{85.4} & \multicolumn{2}{c|}{45.7} \\
		& DRNN~\cite{IrsoyDRNN} & \multicolumn{2}{c|}{86.6} & \multicolumn{2}{c|}{49.8}  \\
		& TE-RNTN~\cite{tag-embedding-rnn} & \multicolumn{2}{c|}{87.7} & \multicolumn{2}{c|}{48.9} \\
		& Dependency Tree-LSTM~\cite{treeLSTM}  & \multicolumn{2}{c|}{85.7 (0.4)*} & \multicolumn{2}{c|}{48.4 (0.4)*}  \\
		& Constituency Tree-LSTM~\cite{treeLSTM} &  \multicolumn{2}{c|}{88.0 (0.3)*}  & \multicolumn{2}{c|}{51.0 (0.5)*} \\
		% & Constituency Tree-LSTM Ensemble~\cite{LooksHHN17} & \multicolumn{2}{c|}{90.2} & \multicolumn{2}{c|}{\underline{53.6}} \\
		\hline
		\multirow{3}{*}{D} & GICF~\cite{group-instance} & \multicolumn{2}{c|}{85.7}  &  \multicolumn{2}{c|}{-} \\
		& Paragraph-Vec~\cite{ParagraphVec} & \multicolumn{2}{c|}{87.8} & \multicolumn{2}{c|}{48.7} \\
		& LSTM (PARAGRAM-SL999)~\cite{wieting2015towards} & \multicolumn{2}{c|}{89.2} & \multicolumn{2}{c|}{-}
		\\
		\hline
		\multirow{2}{*}{E}  & CNN-GRU-word2vec~\cite{cnn-rnn}                    &\multicolumn{2}{c|}{89.95} & \multicolumn{2}{c|}{50.68} \\
		& CNN-LSTM-word2vec~\cite{cnn-rnn}   &       \multicolumn{2}{c|}{89.56} & \multicolumn{2}{c|}{51.50} \Bstrut    \\
		\Xhline{3\arrayrulewidth}
		\Xhline{3\arrayrulewidth}
		    &   & \textbf{Mean(std)} & \textbf{Max} & \textbf{Mean(std)} & \textbf{Max}  \\
		 \cline{3-6}
		\multirow{6}{*}{F} & Constituency Tree-LSTM ~\cite{treeLSTM} (Glove Amazon) & 88.85 (0.44) & 89.35 & 50.53 (0.98) & 51.31 \Tstrut \\
		 & CNN-LSTM                                 & 89.10 (0.39)  & 89.40 & 51.92 (0.63) & 52.66 \\
		  & CNN-LSTM (Glove Amazon) & 89.25 (0.73) & \textbf{90.39}  & 50.84 (0.79) & 51.85 \\
		& 2-channel CNN-LSTM                        & 89.44    (0.51) & 90.01 & 51.70 (0.57) & 52.53 \\
		 & CNN-Tree-LSTM                            & 88.82 (0.13) & 88.92 & 51.35 (1.45) & 52.94 \\
		& CNN-Tree-LSTM (Glove Amazon)             & 88.96 (0.24) & 89.18 & 51.51 (0.99) & 52.80 \\
		& 2-channel CNN-Tree-LSTM  & \textbf{89.70 (0.36)} & 90.12  & \textbf{52.46 (0.55)} & \textbf{53.03} \Bstrut  \\
		\hline
	\end{tabular}
\end{table*}
Experiment results are summaries in Table \ref{table:experimentresult}.
Table \ref{table:experimentresult} contains two parts.
The first part is from Block A to E, which contains all baselines model.
The second part is Block F, which contains all models which were proposed and evaluated by us.
\begin{description}
	\item[Block A] contains convolution neural networks.
	CNN-non-static and CNN-multichannel~\cite{KimCNN} are single layer CNN.
	DCNN~\cite{DCNN} and MVCNN~\cite{2-layer-cnn} are multilayer CNNs, with MVCNN is a large model  which has 2 layers and 5 input channels.
	\item[Block B] contains recurrent neural network models and their variations.
	All the models in this Block B process sentences sequentially.
	DMN~\cite{attention-gru} is a sophisticated model used GRU with attention mechanism and episodic memory.
	\item[Block C] contains models which belong to the family of Recursive Neural Networks (tree-structured models).
	RNTN~\cite{socher2013recursive} is the first recursive neural network to successfully apply on sentence-level sentiment analysis (Stanford Sentiment Treebank).
	DRNN~\cite{IrsoyDRNN} is a multilayered extension of RNTN.
	TE-RNTN is also an extension of RNTN which utilize the local syntactic information at each node of a sentence's parse tree.
	% Constituency Tree-LSTM Ensemble~\cite{LooksHHN17} is an ensemble of 30 Constituency Tree-LSTMs.
	% The models is state-of-the-art system on fine-grained setting of Stanford Sentiment Treebank.
	\item[Block D] contains transfer learning methods, which utilized a large amount of data other than Stanford Sentiment Treebank.
	GICF~\cite{group-instance} learns to classify sentiments of sentences (in Stanford Sentiment Treebank) using only document-level sentiment labels training dataset.
	Paragraph-Vec~\cite{ParagraphVec} learns to encode any sequence of words into a vector with the purpose of maximizing the likelihood of words which appear in that sequence given the encoding vector.
	\item[Block E] contains models which combine Convolution Neural Networks and Recurrent Neural Networks.
\end{description}
\subsection{Discussion}
\subsubsection{Combination of Convolution and Recursive Neural Networks}
The fact that CNN-Tree-LSTM is able to outperforms Constituency Tree-LSTM~\cite{treeLSTM} and CNN-multichannel\cite{KimCNN} supports our hypothesis on the benefits of combining convolution layers with Tree-LSTM.
Additionally, the combination of CNN and LSTM or TreeLSTM outperform most Convolution Network Networks in Block A.
These results support our hypothesis that max pooling layer can be harmful for CNN by ignoring the position of features.
\subsubsection{Glove Amazon versus Standard Glove}
On binary setting, Glove Amazon helps single input-channel models to achieve higher accuracy compared to the standard Glove.
However, on fine-grained setting, Glove Amazon is worse than the standard Glove in most cases except for CNN-Tree-LSTM.
In short, Glove Amazon is good for models on binary setting but can be harmful for them in fine-grained setting.
Because Glove Amazon is trained on review dataset, words which are used to express the same sentiment are more likely to co-occur. 
Therefore, the Glove Amazon word vectors is more dependent on the sentiment expressed in the words compared to the word vectors in the standard Glove which capture more general meaning of words.
We think this is the reason why Glove Amazon beneficial for models to discriminate between positive and negative sentiments in binary setting and at the same time, thwarts the models to discriminate between neutral from the other classes in fine-grained setting. 

A combination of Glove Amazon and the standard Glove improves the accuracy of both 2-channel CNN-LSTM and 2-channel CNN-Tree-LSTM on fine-grained setting as well as 2-channel CNN-Tree-LSTM on binary setting.
Yet, the improvements of CNN-Tree-LSTM are more significant compared to CNN-LSTM.

We also used PARAGRAM-SL999 for initializing the word embedding layer of Constituency Tree-LSTM.
The results (of 8 runs with mean 87.175\% and standard deviation 0.69) were not as good as those of standard Glove.
\subsubsection{Tree-structured Versus Sequential Models}
With single input channel, CNN-LSTM outperforms CNN-Tree-LSTM in most cases except the case of using Glove Amazon on fine-grained setting.

With two input channels using both Glove Amazon and the standard Glove, CNN-Tree-LSTM gains a large improvement and outperforms 2-channel CNN-LSTM on both binary and fine-grained setting.
On average, 2-channel CNN-Tree-LSTM achieves the highest accuracy among all our models on both settings.

