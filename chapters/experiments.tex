\section{Experiments}
\subsection{Datasets}\label{sec:sst}
We evaluated our model on Standford Sentiment Treebank dataset~\cite{socher2013recursive}.
Standford Sentiment Treebank contains total 11,855 sentences.
We used provided train/dev/test sets which contain 8544, 1101 and 2210 sentences, respectively.
In this dataset, every sentence was parsed using Stanford (constituency) parser~\cite{socher2013recursive} into multiple phrases.
There are a total of 215,154 labeled phrases in the whole dataset. Thus, every sentence in the corpus has a fully labeled parse tree.
For training a Recurrent Neural Network, any phrase spanned by a labeled node is treated as a training sample. 

\paragraph{Fine-grained setting} We partitioned sentiment labels into 5 classes: ``Positive'', ``Somewhat Positive'', ``Neutral'', ``Somewhat Negative'' and ``Negative''.

\paragraph{Binary setting} We removed all ``Neutral'' sentences.
For the remaining 6920/872/1821 sentences in train/dev/test sets, we merged ``Somewhat Positive'' into ``Positive'' and ``Somewhat Negative'' into ``Negative''.

\subsection{Setups}
\subsubsection{Experiment Descriptions}
We did experiment with Glove Amazon and different variations of our model.
\begin{description}
	\item[CNN-Tree-LSTM] Our basic model with only one input channel.
	We initialized word representations with the standard Glove vectors.
	\item[CNN-Tree-LSTM (Glove Amazon)] CNN-Tree-LSTM with word vectors initialized from Glove Amazon.
	\item [2-channel CNN-Tree-LSTM] CNN-Tree-LSTM with two input channels from two different word embedding matrices, which are initialized from Glove Common Craw and Glove Amazon, respectively.
	\item[CNN-LSTM] Similar to CNN-Tree-LSTM. However, we replaced Constituency Tree-LSTM module by an LSTM unit.
	CNN-LSTM has only one input channel initialized from the standard Glove vectors.
	\item [CNN-LSTM (Glove Amazon)] CNN-LSTM with word vectors initialized from Glove Amazon.
	\item [2-channel CNN-LSTM] A CNN-LSTM model with two input channels at CNN layers. The CNN layers are similar to CNN layers in 2-channel CNN-Tree-LSTM model.
	\item[Constituency Tree-LSTM (Glove Amazon)]Glove Amazon~is used to replace the standard Glove vector for initializing~word embedding layer of Constituency Tree-LSTM.
	Apart from that, the whole training process and hyper-parameters of Constituency Tree-LSTM~\cite{treeLSTM} are kept unchanged.
	The case of Tree-LSTM using both Glove Amazon, standard Glove were not experimented on because we have found no reliable way to extend the original Tree-LSTM for multi-channel input.
\end{description}

Since the cost functions of neural networks are nonconvex and algorithms used to train neural networks are only able to find local optimum, different runs of one model can converge in various local optimums depend on the initialized parameters of the models.
Base on the evaluating method used by Tai et al.~\cite{treeLSTM}, we evaluated the above models based on mean, standard deviation of 5 runs.
In addition, the maximum accuracy among 5 runs is also reported.

We index all our experimented models along with their number of parameters in Table.\ref{table:paramtable}.
\begin{table}[H]
	\centering
	\caption{Size of memory cell \(r\) and number of trainable parameters \(\left\vert{\theta}\right\vert\) of our models.
	}
	\label{table:paramtable}
	\begin{tabular}{|l|l|l|}
		\hline
		Model & \(r\) & \multicolumn{1}{|c|}{\(\left\vert{\theta}\right\vert\)}\\ \hline
		CNN-LSTM                 & 168         & 489,347          \\
		CNN-Tree-LSTM            & 150         & 482,153          \\
		2-channel CNN-LSTM       & 168         & 729,347          \\
		2-channel CNN-Tree-LSTM  & 150         &
		722,153 \\
		\hline
	\end{tabular}
\end{table}
\subsubsection{Hyper-parameters and Training}
We trained models on training set and tuned hyper-parameters on development set of Stanford .
Our models was trained using AdaGrad~\cite{duchi2011adaptive} with learning rate of $\{0.1, 0.05, 0.01\}$, L2 regularization strength of $\{1e^{-3},~ 1e^{-4}, ~ 1e^{-5} \}$ and batch size of 25.
Word vectors are updated with learning rate $\alpha$ of $\{0.1,~0.05, ~0.01\}$.
In convolution layers, treated number of filter and filter size as hyper-parameters.
We found that 100 filters of size 3 and 100 filters of size 5 yield better results compared to single filters size or the number of filters larger than 200.
We regularized the convolution layers with input dropout rate of 0.5, and output dropout rate of 0.2. At output layer, we regularized with dropout rate of 0.5.
Training with Adagrad's learning rate of 0.01 and word vectors' learning rate 0.1 give the best result.

Our models were trained for 60 epochs.
\subsection{Results}\label{sec:result}
\begin{table*}[]
	\centering
	\caption[Experiment result on SST]{
		Experiment results of models evaluated on Stanford Sentiment Treebank.
		The accuracies of models in blocks A to E are taken from their original papers.
		We highlight the best results among our models and underline the state-of-the-art results.
		\textit{(*): Mean and standard deviation of 5 runs.}
		\textit{(**): Mean of 100 runs, std was not reported.}
	}
	\label{table:experimentresult}
	\begin{tabular}{|c|l|ll|ll|}
		\hline
		\textbf{Block} & \textbf{Model}  & \multicolumn{2}{c|}{\textbf{Binary}} & \multicolumn{2}{c|}{\textbf{Fine-grained}}  \\
		\Xhline{3\arrayrulewidth}
		\Xhline{3\arrayrulewidth}

		\multirow{4}{*}{A} & CNN-non-static~\cite{KimCNN} & \multicolumn{2}{c|}{87.2} & \multicolumn{2}{c|}{48.0} \Tstrut \\
		& CNN-multichannel~\cite{KimCNN} & \multicolumn{2}{c|}{88.1} & \multicolumn{2}{c|}{47.4} \\
		& DCNN~\cite{DCNN} & \multicolumn{2}{c|}{86.8} & \multicolumn{2}{c|}{48.5} \\
		& MVCNN~\cite{2-layer-cnn} & \multicolumn{2}{c|}{89.4} & \multicolumn{2}{c|}{49.6} \\
		\hline
		\multirow{6}{*}{B} & LSTM~\cite{treeLSTM}   & \multicolumn{2}{c|}{84.9 (0.6)*} & \multicolumn{2}{c|}{46.4 (1.1)*} \\
		& BiLSTM~\cite{treeLSTM}  & \multicolumn{2}{c|}{87.5 (0.5)*} & \multicolumn{2}{c|}{49.1 (1.0)*}   \\
		& 2-layer LSTM~\cite{treeLSTM} & \multicolumn{2}{c|}{86.3 (0.6)*} & \multicolumn{2}{c|}{46.0 (1.3)*} \\
		& 2-layer Bidirectional LSTM~\cite{treeLSTM} & \multicolumn{2}{c|}{87.2 (1.0)*} & \multicolumn{2}{c|}{48.5 (1.0)*} \\
		& DMN~\cite{attention-gru} & \multicolumn{2}{c|}{88.6 } & \multicolumn{2}{c|}{52.1} \\
		& Byte mLSTM~\cite{mlstm} & \multicolumn{2}{c|}{\underline{91.80}**} & \multicolumn{2}{c|}{52.90**} \\
		\hline
		\multirow{6}{*}{C} & RNTN~\cite{socher2013recursive}  & \multicolumn{2}{c|}{85.4} & \multicolumn{2}{c|}{45.7} \\
		& DRNN~\cite{IrsoyDRNN} & \multicolumn{2}{c|}{86.6} & \multicolumn{2}{c|}{49.8}  \\
		& TE-RNTN~\cite{tag-embedding-rnn} & \multicolumn{2}{c|}{87.7} & \multicolumn{2}{c|}{48.9} \\
		& Dependency Tree-LSTM~\cite{treeLSTM}  & \multicolumn{2}{c|}{85.7 (0.4)*} & \multicolumn{2}{c|}{48.4 (0.4)*}  \\
		& Constituency Tree-LSTM~\cite{treeLSTM} &  \multicolumn{2}{c|}{88.0 (0.3)*}  & \multicolumn{2}{c|}{51.0 (0.5)*} \\
		& Constituency Tree-LSTM Ensemble~\cite{LooksHHN17} & \multicolumn{2}{c|}{90.2} & \multicolumn{2}{c|}{\underline{53.6}} \\
		\hline
		\multirow{3}{*}{D} & GICF~\cite{group-instance} & \multicolumn{2}{c|}{85.7}  &  \multicolumn{2}{c|}{-} \\
		& Paragraph-Vec~\cite{ParagraphVec} & \multicolumn{2}{c|}{87.8} & \multicolumn{2}{c|}{48.7} \\
		& LSTM (PARAGRAM-SL999)~\cite{wieting2015towards} & \multicolumn{2}{c|}{89.2} & \multicolumn{2}{c|}{-}
		\\
		\hline
		\multirow{2}{*}{E}  & CNN-GRU-word2vec~\cite{cnn-rnn}                    &\multicolumn{2}{c|}{89.95} & \multicolumn{2}{c|}{50.68} \\
		& CNN-LSTM-word2vec~\cite{cnn-rnn}   &       \multicolumn{2}{c|}{89.56} & \multicolumn{2}{c|}{51.50} \Bstrut    \\
		\Xhline{3\arrayrulewidth}
		\Xhline{3\arrayrulewidth}
		&   & \textbf{Mean(std)} & \textbf{Max} & \textbf{Mean(std)} & \textbf{Max}  \\
		\cline{3-6}
		\multirow{6}{*}{F} & Constituency Tree-LSTM ~\cite{treeLSTM} (Glove Amazon) & 88.85 (0.44) & 89.35 & 50.53 (0.98) & 51.31 \Tstrut \\
		& CNN-LSTM                                 & 89.10 (0.39)  & 89.40 & 51.92 (0.63) & 52.66 \\
		& CNN-LSTM (Glove Amazon) & 89.25 (0.73) & \textbf{90.39}  & 50.84 (0.79) & 51.85 \\
		& 2-channel CNN-LSTM                        & 89.44    (0.51) & 90.01 & 51.70 (0.57) & 52.53 \\
		& CNN-Tree-LSTM                            & 88.82 (0.13) & 88.92 & 51.35 (1.45) & 52.94 \\
		& CNN-Tree-LSTM (Glove Amazon)             & 88.96 (0.24) & 89.18 & 51.51 (0.99) & 52.80 \\
		& 2-channel CNN-Tree-LSTM  & \textbf{89.70 (0.36)} & 90.12  & \textbf{52.46 (0.55)} & \textbf{53.03} \Bstrut  \\
		\hline
	\end{tabular}
\end{table*}
Experiment results are summaries in Table \ref{table:experimentresult}.
Table \ref{table:experimentresult} contains two parts.
Block A to E contain all baselines model.
Block F contains all models we proposed and evaluated.
\begin{description}
	\item[Block A] contains convolution neural networks.
	CNN-non-static and CNN-multichannel~\cite{KimCNN} are single layer CNN.
	DCNN~\cite{DCNN} and MVCNN~\cite{2-layer-cnn} are multilayer CNNs, with MVCNN is a large model which has 2 layers and 5 input channels.
	\item[Block B] contains recurrent neural network models and their variations.
	All the models in this Block B process sentences sequentially.
	DMN~\cite{attention-gru} is a sophisticated model used GRU with attention mechanism and episodic memory.
	Byte mLSTM~\cite{mlstm} is the state-of-the-art system on binary setting.
	\item[Block C] contains models which belong to the family of Recursive Neural Networks (tree-structured models).
	RNTN~\cite{socher2013recursive} is the first recursive neural network to successfully apply on sentence-level sentiment analysis (Stanford Sentiment Treebank).
	DRNN~\cite{IrsoyDRNN} is a multilayered extension of RNTN.
	TE-RNTN is also an extension of RNTN which utilize the local syntactic information at each node of a sentence's parse tree.
	Constituency Tree-LSTM Ensemble~\cite{LooksHHN17} is an ensemble of 30 Constituency Tree-LSTMs.
	This model is the state-of-the-art system on the fine-grained setting of Stanford Sentiment Treebank.
	\item[Block D] contains transfer learning methods, which utilized a large amount of data other than Stanford Sentiment Treebank.
	GICF~\cite{group-instance} learns to classify sentiments of sentences (in Stanford Sentiment Treebank) using only document-level sentiment labels training dataset.
	Paragraph-Vec~\cite{ParagraphVec} learns to encode any sequence of words into a vector with the purpose of maximizing the likelihood of words which appear in that sequence given the encoding vector.
	\item[Block E] contains models which combine Convolution Neural Networks and Recurrent Neural Networks.
\end{description}
\subsection{Discussion}
\subsubsection{Combination of Convolution and Recursive Neural Networks}
The fact that CNN-Tree-LSTM outperforms Constituency Tree-LSTM~\cite{treeLSTM} and CNN-multichannel~\cite{KimCNN} supports our hypothesis on the benefits of combining convolution layers with Tree-LSTM.
Additionally, the combination of CNN and LSTM or TreeLSTM outperforms most Convolution Network Networks in Block A. Furthermore, the results support our hypothesis that max pooling layer can be harmful to CNN by ignoring the position of features.


\subsubsection{Glove Amazon versus standard Glove}
\begin{figure} []
	\centering
	\input{figure/plot3.tikz}
	\caption[qwerty]{Mean accuracy of 5 runs on binary setting of Tree-LSTM, CNN-LSTM and CNN-Tree-LSTM using different word embeddings}
	\label{graph:binary}
\end{figure}
On binary setting (Fig. ~\ref{graph:binary}), Glove Amazon helps single input-channel models to achieve higher accuracy compared to the standard Glove.
\begin{figure} []
	\centering
	\input{figure/plot4.tikz}
	\caption[qwerty]{Mean accuracy of 5 runs on fine-grained setting of Tree-LSTM, CNN-LSTM and CNN-Tree-LSTM using different word embeddings}
	\label{graph:fine-grained}
\end{figure}
However, on fine-grained setting (Fig. ~\ref{graph:fine-grained}), Glove Amazon is worse than the standard Glove in most cases except for CNN-Tree-LSTM.
In short, Glove Amazon is good for models on binary setting however harmful in the fine-grained setting.
%The reason is that our Glove Amazon is trained on review dataset, words which are used to express the same sentiment are more likely to co-occur.
The reason is that in review dataset that we trained Glove Amazon on, words which are used to express the same sentiment are more likely to co-occur.
Therefore, the Glove Amazon word vectors are more dependent on the sentiment expressed in the words compared to the word vectors in the standard Glove which capture more general meaning of words.
% This possibly is the reason why Glove Amazon is beneficial for classifying positive and negative sentiments in binary setting but harmful to detecting neutral in the fine-grained setting.

A combination of Glove Amazon and the standard Glove improves the accuracy of both 2-channel CNN-LSTM and 2-channel CNN-Tree-LSTM on fine-grained setting as well as 2-channel CNN-Tree-LSTM on the binary setting.
Yet, the improvements of CNN-Tree-LSTM are more significant compared to CNN-LSTM.
\subsubsection{Tree-structured Versus Sequential Models}
With single input channel, CNN-LSTM outperforms CNN-Tree-LSTM in most cases except the case of using Glove Amazon on the fine-grained setting.
With two input channels using both Glove Amazon and the standard Glove, CNN-Tree-LSTM gains a large improvement and outperforms 2-channel CNN-LSTM on both binary and fine-grained setting.
On average, 2-channel CNN-Tree-LSTM achieves the highest accuracy among all our models in both settings.

\subsubsection{Comparing to the State-of-the-art Models}
Constituency Tree-LSTM Ensemble~\cite{LooksHHN17} is the state-of-the-art model on the fine-grained setting.
Although the performance of 2-channel CNN-Tree-LSTM is lower then Constituency Tree-LSTM Ensemble, Constituency Tree-LSTM Ensemble is not an novel model and is an ensemble of \(30\) Constituency Tree-LSTMs.
Since, the main purpose of Looks et al. were to use Constituency Tree-LSTM as an example to demonstrate the concise and batch-wise parallelism of its TensorFlow Fold implementation~\cite{LooksHHN17}.

2-channel CNN-Tree-LSTM is comparable with Byte mLSTM~\cite{mlstm} on the fine-grained setting but underperforms Byte mLSTM in the binary setting.
Similar to our approach, Radford st al. did transfer learning from large review dataset (which is also Amazon Reviews dataset) to improve the performance of their models on Stanford Sentiment Treebank.
But different from our work, they trained a byte-level multiplicative LSTM~\cite{KrauseLMR16} language model on Amazon Reviews dataset.
After that, the vector representations of all sentences in Stanford Sentiment Treebank were computed using the trained byte-level multiplicative LSTM.
These vector representations along with their corresponding labels were used to train a logistic regression classifier~\cite{mlstm}.

On Stanford Sentiment Treebank, their method achieved state-of-the-art performance on the binary setting and comparable performance with Constituency Tree-LSTM Ensemble on fine-grained setting.
Despite that, the performances of their model on other NLP tasks were not impressive, these tasks include: semantic relatedness (SICK~\cite{SemeEvalTask1}), subjectivity/objectivity detection (SUBJ~\cite{cs-CL-0409058}), opinion polarity (MPQA~\cite{Wiebe2005}) and paraphrase detection (Microsoft Paraphrase Corpus~\cite{Dolan:2004:UCL:1220355.1220406}).
It is likely that these tasks are out-of-domain for their model which only trained on review dataset~\cite{mlstm}.

While conducting this research, we have trained CNN-LSTM as language model on Amazon Reviews dataset.
Nevertheless, we have not gained any success with this method.
