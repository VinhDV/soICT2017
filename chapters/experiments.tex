\section{Experiments}
\subsection{Datasets}
\subsubsection{Stanford Sentiment Treebank} \label{sec:sst}
Standford Sentiment Treebank dataset~\cite{socher2013recursive} was used to evaluate our model.
In total, Standford Sentiment Treebank contains 11,855 sentences.
The dataset was split into training, validation and test set which contains 8544; 1101 and 2210 sentences respectively.
In this dataset, every sentence was parsed using Stanford (constituency) parser~\cite{socher2013recursive}, each phrase which is spanned by any sub-tree of the parse tree is then labeled with  a sentiment label.
There are total of 215,154 labeled phrases in the whole dataset.
For training a Recurrent Neural Network on this dataset, any phrase which is spanned by a labeled node is treated as a training sample. 
\paragraph{Fine-grained setting} Any sentiment label belongs to one of 5 classes: ``Positive'', ``Somewhat Positive'', ``Neutral'', ``Somewhat Negative'' and ``Negative''.
\paragraph{Binary setting} All ``Neutral'' sentences are removed.
For the remaining sentences, ``Somewhat Positive'' is merged into ``Positive'', similarly, ``Somewhat Negative'' is merged into ``Negative''.
As all ``Neutral'' sentences have been removed, there are 6920/872/1821 sentences remained in training/validation/test set.
\subsubsection{Amazon Reviews}\label{sec:amazon}
Amazon Reviews~\cite{amazon-reviews} is a gigantic review dataset
which contains 142.8 million reviews from Amazon spanning May 1996 - July 2014\footnote{\url{http://jmcauley.ucsd.edu/data/amazon/}}.
Each review contains product review (rating, text, helpfulness vote) and metadata (descriptions, category information, price, brand, and image features).
The dataset is partitioned into 24 categories (e.g. ``Books'', ``Electronics'', ``Office Products'', ``Movies and TV'').
\subsection{Setups}
\subsubsection{Glove Amazon}\label{sec:glove-amazon}
We have five steps to preprocess Amazon dataset for training word vectors using Glove\footnote{Publicly available on Github \url{https://github.com/stanfordnlp/GloVe}}:
\begin{enumerate}
	\item For retraining Glove vectors, we only used some partitions of Amazon Reviews dataset which includes:  ``Amazon Movies and TV'' (7,850,072 reviews)~\cite{mcauley2013hidden} and ``Books'' (22,507,155 reviews)~\cite{McAuleyTSH15}~\cite{HeM16}.
	\item All the reviews were grouped by product-ID ( "asin" keyword in the JSON schema of the dataset).
	\item In each product-ID group, the reviews were sorted increasingly by their ratings ("overall" keyword in the JSON schema of the dataset).
	\item All the reviews were dumped into a plain text file.
	\item The text file produced from the previous step was tokenized using Stanford Tokenizer~\cite{tokenizerpart}.
\end{enumerate}

The reason for doing step 2 and step 3 is because there is no definition of end-of-document in Glove model, which means words which appear in the beginning part of a document will be included in the context of words in the last part of the previous document which leads to noise in training data. Step 2 and 3 help us to mitigate this problem.

We set $x_{max} = 100$, vector size to 300, windows size to 20 and the minimum number of word occurrences to be included in the vocabulary to 5.
The training process took the plain text file produced by the preprocessing steps as its input.
In total, the size of the corpus is 4.7 billions tokens.
After the training process, the resulting word embedding has vocabulary size of 1,734,244.
We named this new word embedding Glove Amazon.
\subsubsection{Experiment Descriptions}
We did experiment with Glove Amazon and different variations of our model.
\begin{description}
	\item[Constituency Tree-LSTM (Glove Amazon)] In this model, Glove Amazon is used to replace the standard Glove vector for initializing word embedding layer of Constituency Tree-LSTM.
	Apart from that, the whole training process and hyper-parameters of Constituency Tree-LSTM~\cite{treeLSTM} are kept unchanged.
	\item[CNN-Tree-LSTM] Our basic model which has only one input channel. 
	This input channel is initialized using the standard Glove vectors.
	\item[CNN-Tree-LSTM (Glove Amazon)] CNN-Tree-LSTM but its input channel is initialized using Glove Amazon.
	\item [2-channel CNN-Tree-LSTM] A model with two input channels, one is initialized using the standard Glove while Glove Amazon is used for the other.
	\item[CNN-LSTM] Similar to CNN-Tree-LSTM but the Constituency Tree-LSTM module is replaced by a LSTM unit.
	This model has only one input channel which is initialized using the standard Glove vectors.
	\item [CNN-LSTM (Glove Amazon)] CNN-LSTM but its input channel is initialized using Glove Amazon.
	\item [2-channel CNN-LSTM] CNN-LSTM but having two input channels, one is initialized with the standard Glove vectors and the other is initialized with Glove Amazon.
\end{description}
We also re-run several related models and methods for evaluating their performances (5 runs for each model).
\begin{description}
	\item[LSTM (PARAGRAM-SL999)~\cite{wieting2015towards}] A LSTM model with its word embedding layer initialized by PARAGRAM-SL999 which is a word embedding trained on a large paraphrase dataset (PPDB~\cite{ganitkevitch2013ppdb})
	In our experiments with LSTM (PARAGRAM-SL999), we used the implementation and pre-trained word vectors which are publicly available on the author website\footnote{\url{http://ttic.uchicago.edu/~wieting/}}.
	\item [CNN-LSTM-word2vec~\cite{cnn-rnn}] This model is similar to our CNN-LSTM but a max pooling layer is added after the convolution layer.  
	The pooling layer helps to reduce the size of the feature maps by a half.
	Another difference compared to our model is that the word embedding layer of this model is initialized using the pre-trained word2vec vectors\footnote{\url{https://code.google.com/archive/p/word2vec/}}.
	\item [CNN-GRU-word2vec~\cite{cnn-rnn}] Similar to CNN-LSTM-word2vec, but in this model, GRU unit~\cite{gru} is used instead of LSTM unit.
	In our experiments with CNN-GRU-word2vec and CNN-LSTM-word2vec, we used the implementation publicly published by the authors\footnote{\url{https://github.com/ultimate010/crnn}}.
\end{description}
We index all our experimented models along with their number of parameters in Table.\ref{table:paramtable}.
\begin{table}[H]
	\centering
	\caption{Size of memory cell \(r\) and number of trainable parameters \(\left\vert{\theta}\right\vert\) of our models. 
	}
	\label{table:paramtable}
	\begin{tabular}{|l|l|l|}
		\hline
		Model & \(r\) & \multicolumn{1}{|c|}{\(\left\vert{\theta}\right\vert\)}\\ \hline
		CNN-LSTM                 & 168         & 489,347          \\
		CNN-Tree-LSTM            & 150         & 482,153          \\
		2-channel CNN-LSTM       & 168         & 729,347          \\
		2-channel CNN-Tree-LSTM  & 150         & 
		722,153 \\
		\hline
	\end{tabular}
\end{table}
\subsubsection{Hyper-parameters and Training}
Our models was trained using AdaGrad~\cite{duchi2011adaptive} with learning rate of $\{0.1,~ 0.05,~ 0.01\}$, L2 regularization strength of $\{1e^{-3},~ 1e^{-4}, ~ 1e^{-5} \}$ and batch size of 25.
Word vectors are updated with learning rate $\alpha$ of $\{0.1,~0.05, ~0.01\}$.
We regularized the convolution layers with input dropout rate of 0.5 and output dropout rate of 0.2 in addition to dropout of rate 0.5 at output layer.

We found that 100 filters of size 3 and 100 filters of size 5 yield better results compared to single filters size or the number of filters larger than 200. 
Additionally, training with Adagrad's learning rate of 0.01 and word vectors' learning rate 0.1 give the best result. 
Our models were trained for 60 epochs.
\subsection{Results}\label{sec:result}
\begin{table*}[]
	\centering
	\caption[Experiment result on SST]{Experiment results of models evaluated on Stanford Sentiment Treebank.
		The models which have both data of mean(std) and max are models which have been evaluated by us.
		For these models, we report mean, standard deviation and max of 5 runs.
		If a model has data of only mean(std) or only max, the data is taken from its originated research paper.
		\textit{(*): Result reported by the original paper.}
		\textit{(**): Mean of 100 runs, std was not reported}
	}
	\label{table:experimentresult}
	\begin{tabular}{|c|l|ll|ll|}
		\hline
		   ~ & ~  & \multicolumn{2}{c|}{\textbf{Binary}} & \multicolumn{2}{c|}{\textbf{Fine-grained}}  \\
		\cline{3-6}
		\textbf{Block}    & \textbf{Model}  & \textbf{Mean(std)} & \textbf{Max} & \textbf{Mean(std)} & \textbf{Max}  \\
		\Xhline{3\arrayrulewidth}
		\Xhline{3\arrayrulewidth}
		
		\multirow{4}{*}{A} & CNN-non-static~\cite{KimCNN} & - & 87.20 & - & 48.00 \Tstrut \\
		& CNN-multichannel~\cite{KimCNN} & - & 88.10 & - & 47.40\\
		& DCNN~\cite{DCNN} & - & 86.80 & - & 48.50 \\
		& MVCNN~\cite{2-layer-cnn} & - & 89.40 & - & 49.60 \\
		\hline
		\multirow{6}{*}{B} & LSTM~\cite{treeLSTM}   & 84.9 (0.6) & - & 46.40 (1.1)& - \\
		& BiLSTM~\cite{treeLSTM}  & 87.50 (0.5) & - & 49.1 (1.0) & -  \\
		& 2-layer LSTM~\cite{treeLSTM} & 86.30 (0.60) & - & 46.00 (1.30) & -\\
		& 2-layer Bidirectional LSTM~\cite{treeLSTM} & 87.20 (1.00) & - & 48.50 (1.00) & -\\
		& DMN~\cite{attention-gru} & - & 88.60 & - & 52.10 \\
		& Byte mLSTM~\cite{mlstm} & \underline{\underline{91.80}}** & - & - & 52.90 \\
		\hline
		\multirow{6}{*}{C} & RNTN~\cite{socher2013recursive}  & - & 85.40 & - & 45.70 \\
		& DRNN~\cite{IrsoyDRNN} & - & 86.60 & - & 49.80  \\
		& TE-RNTN~\cite{tag-embedding-rnn} & - & 87.70 & - & 48.90\\
		& Dependency Tree-LSTM~\cite{treeLSTM}  & 85.70 (0.40)  & - & 48.40 (0.4) & - \\
		& Constituency Tree-LSTM~\cite{treeLSTM} & 88.00 (0.30)    &  - & 51.0 (0.5) & - \\
		& Constituency Tree-LSTM Ensemble~\cite{LooksHHN17} & - & 90.20 & - & \underline{\underline{53.60}} \\
		\hline
		\multirow{3}{*}{D} & GICF~\cite{group-instance} & - & 85.70 & - & - \\
		& Paragraph-Vec~\cite{ParagraphVec} & - & 87.80 & - & 48.70 \\
		& LSTM (PARAGRAM-SL999)~\cite{wieting2015towards} & 87.98 (0.46) & 88.50 (89.20)* & 00.00 (00.00)  & 00.00 (00.00)*
		\\
		\hline
		\multirow{2}{*}{E}  & CNN-GRU-word2vec~\cite{cnn-rnn}                    & 89.13 (0.29)  &  89.61 (89.95)* & 00.00 (00.00) & 00.00 (00.00)*   \\
		& CNN-LSTM-word2vec~\cite{cnn-rnn}                    & 89.43 (0.28)  & 89.72 (89.56)* & 52.03 (0.77) & 52.56 (51.50)* \Bstrut    \\
		\Xhline{3\arrayrulewidth}
		\Xhline{3\arrayrulewidth}
		\multirow{6}{*}{F} & Constituency Tree-LSTM ~\cite{treeLSTM} (Glove Amazon) & 88.85 (0.44) & 89.35 & 50.53 (0.98) & 51.31 \Tstrut \\
		 & CNN-LSTM                                 & 89.10 (0.39)  & 89.40 & 00.00 (0.00) & 00.00 \\
		  & CNN-LSTM (Glove Amazon) & \textbf{89.81 (0.28)} & \textbf{90.28}  & 00.00 (00.00) & 00.00 \\
		& 2-channel CNN-LSTM                        & 89.54    (0.22) & 89.79 & 00.00 (00.00) & 00.00 \\
		 & CNN-Tree-LSTM                            & 88.82 (0.13) & 88.92 & 51.35 (1.45) & 52.94 \\
		& CNN-Tree-LSTM (Glove Amazon)             & 88.96 (0.24) & 89.18 & 51.51 (0.99) & 52.80 \\
		& 2-channel CNN-Tree-LSTM  & 89.70 (0.36) & 90.12  & \textbf{52.46 (0.55)} & \textbf{53.03} \Bstrut  \\
		\hline
	\end{tabular}
\end{table*}
Experiment results are summaries in Table \ref{table:experimentresult}.
Table \ref{table:experimentresult} contains two parts.
The first part is from Block A to E, which contains all baselines model.
The second part is Block F, which contains all models which were proposed and evaluated by us.
\begin{description}
	\item[Block A] contains convolution neural networks.
	CNN-non-static and CNN-multichannel~\cite{KimCNN} are single layer CNN.
	DCNN~\cite{DCNN} and MVCNN~\cite{2-layer-cnn} are multilayer CNNs, with MVCNN is a large model  which has 2 layers and 5 input channels.
	\item[Block B] contains recurrent neural network models and their variations.
	All the models in this block process sentences sequentially.
	DMN~\cite{attention-gru} is a sophisticated model used GRU with attention mechanism and episodic memory.
	Byte mLSTM~\cite{mlstm} is the state-of-the-art system on binary setting of Stanford Sentiment Treebank.
	\item[Block C] contains models which belong to the family of Recursive Neural Networks (tree-structured models).
	RNTN~\cite{socher2013recursive} is the first recursive neural network to successfully apply on sentence-level sentiment analysis (Stanford Sentiment Treebank).
	DRNN~\cite{IrsoyDRNN} is a multilayered extension of RNTN.
	TE-RNTN is also an extension of RNTN which utilize the local syntactic information at each node of a sentence's parse tree.
	Constituency Tree-LSTM Ensemble~\cite{LooksHHN17} is an ensemble of 30 Constituency Tree-LSTMs.
	This is the state-of-the-art system on fine-grained setting of Stanford Sentiment Treebank.
	\item[Block D] contains transfer learning methods, which utilized a large amount of data other than Stanford Sentiment Treebank.
	GICF~\cite{group-instance} is an attempt to learn to classify sentiments of sentences (in Stanford Sentiment Treebank) using only document-level sentiment labels training dataset.
	Paragraph-Vec~\cite{ParagraphVec} is a method that learns to encode any sequence of words into a vector with the purpose of maximizing the likelihood of words which appear in that sequence given the encoding vector.
	\item[Block E] contains models which combine Convolution Neural Networks and Recurrent Neural Networks.
\end{description}
\subsection{Discussion}
The fact that CNN-Tree-LSTM outperforms Constituency Tree-LSTM~\cite{treeLSTM} and CNN-multichannel\cite{KimCNN} supports our hypothesis on the benefits of combining convolution layers with Tree-LSTM.

Due to using similar architects, CNN-LSTM was comparable with CNN-GRU-word2vec and CNN-LSTM-word2vec~\cite{cnn-rnn}.
The max pooling layer in CNN-LSTM-word2vec might help it to better generalize compares to our CNN-LSTM.

Constituency Tree-LSTM using Glove Amazon largely outperformed Constituency Tree-LSTM using standard Glove even though it was trained on significantly larger dataset (840B tokens) compared to our preprocessed Amazon Reviews (4.7B tokens).
Additionally, Glove Amazon also improved CNN-Tree-LSTM.
We can conclude that Amazon Glove captured some useful features that does not exist or hardly be extracted in standard Glove.
This result support our hypothesis that by training word embedding on review documents, especially movie or book reviews, we can capture more rare words and also the different way that people use words (or different word relationships) when they express their opinions on movies or books.

On the other hand, the number of parameters of 2-channel CNN-Tree-LSTM (722,153) is much larger than that of CNN-Tree-LSTM (482,153), which makes it more likely for 2-channel CNN-Tree-LSTM to over-fit the training data.
However, 2-channel CNN-Tree-LSTM was able to archive much higher accuracy than both CNN-Tree-LSTM (Glove Amazon) and CNN-Tree-LSTM (standard Glove).
For the case of 2-channel CNN-LSTM versus CNN-LSTM an analogous observation can be made.
These results prove that there are some useful features that only appear in the standard Glove or when combining both Glove Amazon and standard Glove.
Otherwise, there would not be any improvement.
The combination of Glove Amazon and standard Glove help 2-channel CNN-LSTM and 2-channel CNN-Tree-LSTM to outperform both CNN-GRU-word2vec and CNN-LSTM-word2vec.

We also used PARAGRAM-SL999 for initializing the word embedding layer of Constituency Tree-LSTM,
the results (of 8 runs with mean 87.175\% and standard deviation 0.69) were not as good as those of standard Glove.

CNN-Tree-LSTM performed worst than CNN-LSTM, the reason might because of over-fitting.
In contrast, 2-channel CNN-Tree-LSTM was able to outperform 2-channel CNN-LSTM.

Compared to state-of-the-art systems, our models underperform Byte mLSTM~\cite{mlstm} and Constituency Tree-LSTM Ensemble~\cite{LooksHHN17} on binary and fine-grained setting, respectively.
Nonetheless, they are able to outperform Byte mLSTM on fine-grained setting and Constituency Tree-LSTM Ensemble on binary setting.

