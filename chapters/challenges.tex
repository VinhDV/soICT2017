\section{Challenges}
\subsection{Convolution and Pooling}\label{sec:cnn-weak}
Although max pooling layer largely simplified the network (which is good for preventing over-fit), this solution have a clear disadvantage.
By down-sampling a feature map, although the resulted vector still contains information about the existence of a feature, it is likely that the information about the position of the feature is ignored, especially for the case of max-over-time pooling.
\subsection{Recursive Neural Network}\label{sec:recursive-nn-weak}
Tree-LSTMs also have some drawbacks, including:
\begin{itemize}
	\item Sentences can be wrongly parsed, especially when comments are expressed in informal language.
	The performance of the system depends on the parser being used.
	\item At their leaf-module, Tree-LSTMs have only a simple logistic regression layer on top of the vector presentation of a single word at that position.
	The simple leaf-module of Tree-LSTMs might be its weakness when dealing with the problem of words ambiguity.
	This weakness becomes even more severe when the sentence is wrongly parsed. 
\end{itemize}
\subsection{Word Embeddings}\label{sec:word-weak}
There are many words (e.g. ``B-rated'', ``Batman'', ``Nolan'', ``cartoonlike'') which rarely appears in regular documents but more often in movie reviews.
These words might not appear or not have good vector representations in the pre-trained Glove Common Crawl.

Additionally, the ways people use words in movie reviews might be different from their usage in general documents.
For example, the vector presentations of ``sympathy'' and ``disappointed'', or ``boom'' and ``insult'' are very close to each other in Glove Common Crawl but if we have to predict the sentiment of a movie comment which has one of these words, they should be distinctive.

Noticeably, many experiments~\cite{treeLSTM, KimCNN} have shown that modifying word embeddings during the training process help improving the performance of Deep Learning systems.
Although this method improves models' performance, it can harm generalization by updating only words which appear in the training set and non-related words which only appear in the test set.