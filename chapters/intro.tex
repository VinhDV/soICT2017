\section{Introduction}
In recent years, thanks to the dramatic growth of social media, customers' opinions are expressed in the highest speed and volume ever recorded in history.
It is inefficient to read, analyze, or even collect such a large amount of data manually.
Sentiment analysis offers a way to collect and process public opinion automatically.
Basically, sentiment analysis is used to determine whether an opinion about a specific product, event, or organization is positive or negative.
Formally, given document $d$, the main objective of sentiment analysis is to extract the following quintuple~\cite{liu2012sentiment}:
\[ ( e_{i}, a_{ij}, s_{ijkl}, h_{k}, t_{l} ) \]
Where:
\begin{itemize}
	\item $e_{i}$: entity \(i\) (entity extraction and categorization)
	\item $a_{ij}$: aspect \(j\) of entity \(i\) (entity extraction and categorization)
	\item $h_{k}$: holder \(k\) (opinion holder extraction and categorization)
	\item $t_{l}$: time \(l\) (time extraction and standardization)
	\item $s_{ijkl}$: opinion of holder \(k\) about aspect \(j\) of entity \(i\) at time \(l\) (aspect sentiment classification)
\end{itemize}
Sentence-level sentiment analysis is to determine whether a sentence expresses positive or negative sentiment.
This level of analysis assumes that every sentence contains one opinion toward an entity (e.g., a single movie)~\cite{liu2012sentiment}.

In this paper, we explore two ideas: ``Combining Convolution and Recursive Neural Networks'' (the main idea) and ``Transfer Learning From Large Review Dataset'' (the supporting idea).
%\begin{description}
%    \item[Combining Convolution and Recursive Neural Networks] Convolution (Section~\ref{sec:cnn}) and Recursive Neural Networks (Section~\ref{sec:recursive-nn}) have been proved to be effective network architects for sentence-level sentiment analysis.
%    Nevertheless, each of them has its own potential drawbacks (Section~\ref{sec:cnn-weak},~\ref{sec:recursive-nn-weak}).
%    For alleviating their weaknesses, we combined Convolution and Recursive Neural Networks into a new network architect (Section~\ref{sec:cnn-treelstm}) which can outperform both Convolution and Recursive Neural Networks (Section~\ref{sec:result}) on Stanford Sentiment Treebank (Section~\ref{sec:sst}).
%    This approach is closely related to the paper of Wang et al.~\cite{cnn-rnn} which investigate the combination Convolution and Recurrent Neural Networks.
%    \item[Transfer Learning From Large Review Dataset] One obstacle to solving sentence-level sentiment analysis is the lack of labeled data which potentially causes many drawbacks, and one of them is over-fitting word embedding (Section~\ref{sec:word-weak}).
%    Since most opinions are expressed in the form of  documents (i.e., multiple sentences), sentence-level labeled dataset required more work to produce.
%    Until now, the largest dataset for sentence-level sentiment analysis is Standford Sentiment Treebank which only contains 11,855 sentences.
%    This number is insignificant compared to Amazon Review dataset (Section~\ref{sec:amazon}) which has 83.68 million reviews.
%    We utilized Amazon Review dataset to train a new word embedding (Section~\ref{sec:glove-amazon}) which was named ``Glove Amazon''.
%    By replacing the standard Glove\footnote{Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download) publicly available at \url{https://nlp.stanford.edu/projects/glove/}} with Glove Amazon, many models can gain considerable improvements when evaluated on Stanford Sentiment Treebank.
%    We also demonstrated that a combination of Glove Amazon and standard Glove is better than each of the word embeddings (Section~\ref{sec:result}).
%\end{description}
\paragraph{Combining Convolution and Recursive Neural Networks} Convolution (CNN) and Recursive Neural Networks (RecNN) have been proven to be effective network architectures for sentence-level sentiment analysis.
Nevertheless, each of them has drawbacks (Section~\ref{sec:related}).
For alleviating their weaknesses, we combined CNN and RecNN into a new network architecture (Section~\ref{sec:cnn-treelstm}) which is able to outperform both CNN and RecNN (Section~\ref{sec:result}) on Stanford Sentiment Treebank (Section~\ref{sec:sst}).
This approach is closely related to the paper of Wang et al.~\cite{cnn-rnn} which investigates the of combination CNN and Recurrent Neural Networks (RNN).
\paragraph{Transfer Learning From Large Review Dataset} One obstacle of solving sentence-level sentiment analysis is the lack of labeled data which potentially causes many drawbacks, one of which is over-fitting word embedding (Section~\ref{sec:we-motive}).
Since most opinions are expressed in form the of  a documents (i.e., multiple sentences), sentence-level labeled dataset required more work to produce.
Until now, the largest dataset for sentence-level sentiment analysis is Standford Sentiment Treebank which only contains 11,855 sentences.
The number is insignificant compared to Amazon Review dataset which has 83.68 million reviews.
We utilized Amazon Review dataset to train a new word embedding (Section~\ref{sec:glove-amazon}) named ``Glove Amazon''.
By replacing the standard Glove\footnote{Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download) publicly available at \url{https://nlp.stanford.edu/projects/glove/}} with Glove Amazon, many models can gain considerable improvements when evaluated on Stanford Sentiment Treebank.
We also demonstrated that a combination of Glove Amazon and standard Glove is better than each of the word embeddings (Section~\ref{sec:result}).

We provide the source code for the model as well as trained word vectors at \url{https://github.com/ttpro1995/Tree_CNN_LSTM}.
